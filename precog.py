# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14k2cNrpJ3iA_H7VkEfL1Lp3DygPjr-Lt
"""

# Cell 1: Import libraries
import os
import random
import string
import pickle
import numpy as np
from PIL import Image, ImageFont, ImageDraw
import cv2
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from google.colab import drive
import urllib.request
import zipfile
import io

# Mount Google Drive (if needed)
# Uncomment this if you want to save data to your Google Drive
# drive.mount('/content/drive')

# Cell 2: Utility functions for data generation
def ensure_dir(dir_path):
    """Make sure the directory exists."""
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

def download_fonts():
    """Download some free fonts for use in Colab."""
    font_urls = [
        "https://github.com/google/fonts/raw/main/apache/roboto/static/Roboto-Regular.ttf",
        "https://github.com/google/fonts/raw/main/apache/roboto/static/Roboto-Bold.ttf",
        "https://github.com/google/fonts/raw/main/ofl/opensans/static/OpenSans-Regular.ttf",
        "https://github.com/google/fonts/raw/main/ofl/opensans/static/OpenSans-Bold.ttf",
        "https://github.com/google/fonts/raw/main/ofl/lato/Lato-Regular.ttf",
        "https://github.com/google/fonts/raw/main/ofl/lato/Lato-Bold.ttf",
        "https://github.com/google/fonts/raw/main/ofl/sourcesanspro/SourceSansPro-Regular.ttf",
        "https://github.com/google/fonts/raw/main/ofl/sourcesanspro/SourceSansPro-Bold.ttf",
        "https://github.com/google/fonts/raw/main/ofl/montserrat/static/Montserrat-Regular.ttf",
        "https://github.com/google/fonts/raw/main/ofl/montserrat/static/Montserrat-Bold.ttf",
    ]

    # Create fonts directory
    fonts_dir = os.path.join(os.getcwd(), 'fonts')
    ensure_dir(fonts_dir)

    # Download fonts
    font_paths = []
    for url in font_urls:
        try:
            font_name = url.split('/')[-1]
            font_path = os.path.join(fonts_dir, font_name)
            if not os.path.exists(font_path):
                urllib.request.urlretrieve(url, font_path)
            font_paths.append(font_path)
            print(f"Downloaded {font_name}")
        except Exception as e:
            print(f"Failed to download {url}: {e}")

    if not font_paths:
        print("Warning: No fonts could be downloaded. Using default font.")
        font_paths = [None]

    return font_paths

def generate_random_text(min_length=3, max_length=9):
    """Generate random alphanumeric text with random length."""
    length = random.randint(min_length, max_length)

    # Use uppercase letters, lowercase, and digits
    chars = string.ascii_uppercase + string.ascii_lowercase + string.digits

    # Generate random text
    text = ''.join(random.choice(chars) for _ in range(length))

    return text

def apply_random_capitalization(text):
    """Randomly capitalize or lowercase letters in the text."""
    result = ""
    for char in text:
        if char.isalpha():  # If it's a letter
            # 50% chance to convert to uppercase, 50% to lowercase
            if random.random() < 0.5:
                result += char.upper()
            else:
                result += char.lower()
        else:  # If it's not a letter (e.g., a digit)
            result += char
    return result

def generate_text_image(text, font_path, img_width, img_height):
    """Generate an image with the given text using the specified font."""
    # Create a blank white image
    image = Image.new('L', (img_width, img_height), color=255)
    draw = ImageDraw.Draw(image)

    # Calculate font size (adjusting down if needed)
    target_height = int(img_height * 0.7)  # Target text height as % of image height
    font_size = target_height

    try:
        font = ImageFont.truetype(font_path, font_size)
    except:
        # Fallback to default font if the specified font fails
        font = ImageFont.load_default()

    # Get text size (different method for newer PIL versions in Colab)
    try:
        # For newer PIL versions
        left, top, right, bottom = font.getbbox(text)
        text_width = right - left
        text_height = bottom - top
    except AttributeError:
        # For older PIL versions
        text_width, text_height = draw.textsize(text, font=font)

    # If text is too wide, scale down the font
    while text_width > img_width * 0.9 and font_size > 8:
        font_size -= 1
        try:
            font = ImageFont.truetype(font_path, font_size)
        except:
            font = ImageFont.load_default()

        try:
            # For newer PIL versions
            left, top, right, bottom = font.getbbox(text)
            text_width = right - left
            text_height = bottom - top
        except AttributeError:
            # For older PIL versions
            text_width, text_height = draw.textsize(text, font=font)

    # Calculate position to center the text
    x = (img_width - text_width) // 2
    y = (img_height - text_height) // 2

    # Draw the text in black using correct method for Colab's PIL version
    try:
        # For newer PIL versions
        draw.text((x, y), text, fill=0, font=font)
    except:
        # For older PIL versions
        draw.text((x, y), text, fill=0, font=font)

    # Apply random distortion for more variability
    if random.random() < 0.3:  # 30% chance of distortion
        # Convert PIL image to numpy array for OpenCV processing
        img_np = np.array(image)

        # Apply slight perspective transformation
        height, width = img_np.shape
        src_pts = np.float32([[0, 0], [width-1, 0], [0, height-1], [width-1, height-1]])

        # Random distortion amount (not too extreme)
        distort_factor = 0.1
        dst_pts = np.float32([
            [0 + random.uniform(0, width * distort_factor),
             0 + random.uniform(0, height * distort_factor)],
            [width-1 - random.uniform(0, width * distort_factor),
             0 + random.uniform(0, height * distort_factor)],
            [0 + random.uniform(0, width * distort_factor),
             height-1 - random.uniform(0, height * distort_factor)],
            [width-1 - random.uniform(0, width * distort_factor),
             height-1 - random.uniform(0, height * distort_factor)]
        ])

        # Apply perspective transform
        M = cv2.getPerspectiveTransform(src_pts, dst_pts)
        img_np = cv2.warpPerspective(img_np, M, (width, height), borderMode=cv2.BORDER_CONSTANT, borderValue=255)

        # Convert back to PIL image
        image = Image.fromarray(img_np)

    # Add some Gaussian noise
    if random.random() < 0.2:  # 20% chance of noise
        img_np = np.array(image)
        noise = np.random.normal(0, 5, img_np.shape).astype(np.uint8)
        img_np = np.clip(img_np + noise, 0, 255).astype(np.uint8)
        image = Image.fromarray(img_np)

    return image

# Cell 3: Generate the dataset with additional test2 folder
def generate_dataset(num_samples=10000, output_dir='./data', img_width=100, img_height=32,
                     train_ratio=0.7, test_ratio=0.15, min_length=3, max_length=9):
    """
    Generate a dataset of text images with the specified parameters.

    Args:
        num_samples: Number of samples to generate
        output_dir: Root directory to save the dataset
        img_width: Width of the generated images
        img_height: Height of the generated images
        train_ratio: Ratio of training samples
        test_ratio: Ratio of test samples (remainder goes to test2)
        min_length: Minimum length of generated text
        max_length: Maximum length of generated text
    """
    # Ensure output directories exist
    train_img_dir = os.path.join(output_dir, 'train', 'text')
    test_img_dir = os.path.join(output_dir, 'test', 'text')
    test2_img_dir = os.path.join(output_dir, 'test2', 'text')  # New test2 directory

    train_dir = os.path.join(output_dir, 'train')
    test_dir = os.path.join(output_dir, 'test')
    test2_dir = os.path.join(output_dir, 'test2')  # New test2 parent directory

    ensure_dir(train_img_dir)
    ensure_dir(test_img_dir)
    ensure_dir(test2_img_dir)  # Ensure test2 directory exists
    ensure_dir(train_dir)
    ensure_dir(test_dir)
    ensure_dir(test2_dir)  # Ensure test2 parent directory exists

    # Get available fonts
    font_paths = download_fonts()
    print(f"Using {len(font_paths)} fonts for generation.")

    # Split samples into train, test and test2
    num_train = int(num_samples * train_ratio)
    num_test = int(num_samples * test_ratio)
    num_test2 = num_samples - num_train - num_test

    # Ground truth dictionaries
    train_gt = {}
    test_gt = {}
    test2_gt = {}  # New ground truth for test2

    # Generate training samples
    print("Generating training samples...")
    for i in tqdm(range(num_train)):
        # Generate random text with random capitalization
        text = generate_random_text(min_length, max_length)
        text = apply_random_capitalization(text)

        # Select a random font
        font_path = random.choice(font_paths)

        # Generate image
        img = generate_text_image(text, font_path, img_width, img_height)

        # Save image
        img_path = os.path.join(train_img_dir, f"{i}.jpg")
        img.save(img_path)

        # Add to ground truth
        train_gt[i] = text

    # Generate test samples
    print("Generating test samples...")
    for i in tqdm(range(num_test)):
        # Generate random text with random capitalization
        text = generate_random_text(min_length, max_length)
        text = apply_random_capitalization(text)

        # Select a random font
        font_path = random.choice(font_paths)

        # Generate image
        img = generate_text_image(text, font_path, img_width, img_height)

        # Save image
        img_path = os.path.join(test_img_dir, f"{i}.jpg")
        img.save(img_path)

        # Add to ground truth
        test_gt[i] = text

    # Generate test2 samples (with more distortion)
    print("Generating test2 samples...")
    for i in tqdm(range(num_test2)):
        # Generate random text with random capitalization
        text = generate_random_text(min_length, max_length)
        text = apply_random_capitalization(text)

        # Select a random font
        font_path = random.choice(font_paths)

        # Generate image
        img = generate_text_image(text, font_path, img_width, img_height)

        # Additional distortions for test2 samples
        img_np = np.array(img)

        # Apply more extreme distortion
        if random.random() < 0.5:
            # More rotation
            angle = random.uniform(-25, 25)  # More extreme angles
            center = (img_np.shape[1] // 2, img_np.shape[0] // 2)
            M = cv2.getRotationMatrix2D(center, angle, 1.0)
            img_np = cv2.warpAffine(img_np, M, (img_np.shape[1], img_np.shape[0]),
                                   borderMode=cv2.BORDER_CONSTANT, borderValue=255)

        # Add more noise
        if random.random() < 0.6:
            noise = np.random.normal(0, 10, img_np.shape).astype(np.uint8)  # More noise
            img_np = np.clip(img_np + noise, 0, 255).astype(np.uint8)

        # Convert back to PIL image
        img = Image.fromarray(img_np)

        # Save image
        img_path = os.path.join(test2_img_dir, f"{i}.jpg")
        img.save(img_path)

        # Add to ground truth
        test2_gt[i] = text

    # Save ground truth dictionaries
    with open(os.path.join(train_dir, 'gt.pkl'), 'wb') as f:
        pickle.dump(train_gt, f)

    with open(os.path.join(test_dir, 'gt.pkl'), 'wb') as f:
        pickle.dump(test_gt, f)

    with open(os.path.join(test2_dir, 'gt.pkl'), 'wb') as f:
        pickle.dump(test2_gt, f)

    print(f"Generated {num_train} training samples, {num_test} test samples, and {num_test2} test2 samples.")
    print(f"Sample training entry - ID: 0, Text: '{train_gt[0]}'")
    if test_gt:
        print(f"Sample test entry - ID: 0, Text: '{test_gt[0]}'")
    if test2_gt:
        print(f"Sample test2 entry - ID: 0, Text: '{test2_gt[0]}'")

    # Display some sample images from all three sets
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))

    # Display training samples
    for i in range(min(3, num_train)):
        img_path = os.path.join(train_img_dir, f"{i}.jpg")
        img = Image.open(img_path)
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].set_title(f"Train: {train_gt[i]}")
        axes[0, i].axis('off')

    # Display test samples
    for i in range(min(3, num_test)):
        img_path = os.path.join(test_img_dir, f"{i}.jpg")
        img = Image.open(img_path)
        axes[1, i].imshow(img, cmap='gray')
        axes[1, i].set_title(f"Test: {test_gt[i]}")
        axes[1, i].axis('off')

    # Display test2 samples
    for i in range(min(3, num_test2)):
        img_path = os.path.join(test2_img_dir, f"{i}.jpg")
        img = Image.open(img_path)
        axes[2, i].imshow(img, cmap='gray')
        axes[2, i].set_title(f"Test2: {test2_gt[i]}")
        axes[2, i].axis('off')

    plt.tight_layout()
    plt.show()

    return train_gt, test_gt, test2_gt

# Cell 4: Run the data generation
# You can modify these parameters as needed
num_samples = 30000  # Reduced for Colab - increase as needed
output_dir = './data'
img_width = 100
img_height = 32
train_ratio = 0.7   # Changed to 70% training
test_ratio = 0.15   # 15% for test
# Remaining 15% will go to test2
min_length = 3
max_length = 9

# Generate the dataset with the additional test2 folder
train_gt, test_gt, test2_gt = generate_dataset(
    num_samples=num_samples,
    output_dir=output_dir,
    img_width=img_width,
    img_height=img_height,
    train_ratio=train_ratio,
    test_ratio=test_ratio,
    min_length=min_length,
    max_length=max_length
)

# Testing and visualization of CRNN model results
import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import os

# Load previous model and dataset classes
#from SimplifiedCRNN import SimplifiedCRNN, OCRDataset, get_character_set, collate_fn

def test_model(model_path, num_samples=20, display_results=True):
    """
    Test the model on validation set and display results
    """
    # Configuration
    batch_size = 32
    num_lstm_layer = 2
    num_label = 9
    data_shape = (100, 32)  # (W, H)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Get character classes
    classes = get_character_set()
    num_classes = len(classes) + 1  # +1 for CTC blank

    # Create test dataset and dataloader
    test_dataset = OCRDataset(batch_size, classes, data_shape, num_label, train_flag=False)
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,  # Use batch size of 1 for testing individual samples
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )

    # Load checkpoint first to determine hidden size
    checkpoint = torch.load(model_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    else:
        state_dict = checkpoint

    # Extract hidden dimension from checkpoint
    lstm_hidden_size = state_dict['rnn.weight_hh_l0'].shape[1]  # Should be 256
    print(f"Detected hidden size from checkpoint: {lstm_hidden_size}")

    # Create model with correct dimensions from checkpoint
    model = SimplifiedCRNN(num_classes, num_hidden=lstm_hidden_size, num_lstm_layers=num_lstm_layer)

    # Load model weights
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(state_dict)
        print(f"Loaded model from epoch {checkpoint['epoch']} with validation accuracy {checkpoint['val_acc']:.4f}")
    else:
        model.load_state_dict(state_dict)
        print("Loaded model weights successfully")

    model = model.to(device)
    model.eval()

    # Evaluate samples
    correct = 0
    total = 0
    results = []

    print("\nEvaluating model...")
    with torch.no_grad():
        for i, (images, labels, label_lengths) in enumerate(test_loader):
            if i >= num_samples:
                break

            images = images.to(device)

            # Forward pass
            outputs = model(images)
            outputs = outputs.permute(1, 0, 2)  # (batch, seq_len, num_classes)

            # Get predictions
            pred_indices = torch.argmax(outputs, dim=2)  # (batch, seq_len)

            # CTC decoding
            decoded_pred = []
            for j in range(pred_indices.size(0)):
                pred = pred_indices[j].cpu().numpy()
                decoded = []
                for k in range(pred.shape[0]):
                    if pred[k] != 0 and (len(decoded) == 0 or pred[k] != decoded[-1]):  # CTC decoding
                        decoded.append(pred[k])
                decoded_pred.append(decoded)

            # Convert indices to characters
            pred_texts = []
            for pred in decoded_pred:
                text = ''.join([classes[p-1] if p > 0 and p <= len(classes) else '' for p in pred])
                pred_texts.append(text)

            # Get ground truth texts
            gt_texts = []
            for j in range(labels.size(0)):
                length = label_lengths[j].item()
                text = ''.join([classes[labels[j][k]-1] if labels[j][k] > 0 else '' for k in range(length)])
                gt_texts.append(text)

            # Calculate accuracy
            for j in range(len(gt_texts)):
                if pred_texts[j] == gt_texts[j]:
                    correct += 1
                total += 1

            # Store results for display
            results.append({
                'image': images[0].cpu().numpy(),
                'pred': pred_texts[0],
                'gt': gt_texts[0]
            })

    accuracy = correct / total if total > 0 else 0
    print(f"Test Accuracy: {accuracy:.4f} ({correct}/{total})")

    # Display results
    if display_results:
        plt.figure(figsize=(15, 5 * min(len(results), 10)))

        for i, result in enumerate(results[:10]):  # Display at most 10 samples
            plt.subplot(min(len(results), 10), 1, i + 1)

            # Display image
            img = result['image'][0]  # Get first channel
            plt.imshow(img, cmap='gray')

            # Display text
            title = f"Prediction: '{result['pred']}'"
            if result['pred'] == result['gt']:
                title += f" ✓ (Ground Truth: '{result['gt']}')"
            else:
                title += f" ✗ (Ground Truth: '{result['gt']}')"

            plt.title(title)
            plt.axis('off')

        plt.tight_layout()
        plt.show()

    return accuracy, results
# Run test if this is the main script
if __name__ == "__main__":
    # Path to your trained model
    model_path = os.path.join('model', 'crnn_best.pth')

    if not os.path.exists(model_path):
        print(f"Model file not found: {model_path}")
        model_paths = [f for f in os.listdir('model') if f.endswith('.pth')]
        if model_paths:
            model_path = os.path.join('model', model_paths[0])
            print(f"Using available model instead: {model_path}")
        else:
            print("No model files found in 'model' directory.")
            exit(1)

    test_model(model_path, num_samples=50, display_results=True)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import os
import cv2
import numpy as np
import pickle
import random
from torch.utils.data import Dataset, DataLoader

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
    device = torch.device("cuda")
    print(f"Using CUDA: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("CUDA not available. Using CPU.")

# Dataset Class
class OCRDataset(Dataset):
    def __init__(self, batch_size, classes, data_shape, num_label, train_flag=True):
        self.batch_size = batch_size
        self.data_shape = data_shape
        self.num_label = num_label
        self.classes = classes

        # Set paths for data
        if train_flag:
            self.data_path = os.path.join(os.getcwd(), 'data', 'train', 'text')
            self.label_path = os.path.join(os.getcwd(), 'data', 'train')
        else:
            self.data_path = os.path.join(os.getcwd(), 'data', 'test', 'text')
            self.label_path = os.path.join(os.getcwd(), 'data', 'test')

        # Load image indices and labels
        self.image_set_index = self._load_image_set_index(shuffle=train_flag)
        self.gt = self._label_path_from_index()

    def __len__(self):
        return len(self.image_set_index)

    def __getitem__(self, idx):
        img_name = self.image_set_index[idx]
        img_path = os.path.join(self.data_path, img_name + '.jpg')

        # Read and preprocess image
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            print(f"Warning: Could not read image {img_path}")
            img = np.zeros((self.data_shape[1], self.data_shape[0]), dtype=np.uint8)

        img = cv2.resize(img, self.data_shape)
        img = img.reshape((1, self.data_shape[1], self.data_shape[0]))  # (C, H, W)
        img = np.multiply(img, 1/255.0)

        # Get label
        plate_str = self.gt[int(img_name)]
        label = np.zeros(self.num_label, dtype=np.int32)
        for i in range(len(plate_str)):
            if plate_str[i] in self.classes:
                label[i] = self.classes.index(plate_str[i]) + 1  # +1 because 0 is reserved for blank in CTC
            else:
                print(f"Warning: Unknown character '{plate_str[i]}' in sample {img_name}")
                label[i] = 0

        return torch.FloatTensor(img), torch.LongTensor(label), min(len(plate_str), self.num_label)

    def _load_image_set_index(self, shuffle):
        assert os.path.isdir(self.data_path), f'Path does not exist: {self.data_path}'
        image_set_index = []
        list_dir = os.walk(self.data_path)
        for root, _, image_names in list_dir:
            for name in image_names:
                image_set_index.append(name.split('.')[0])
        if shuffle:
            random.shuffle(image_set_index)
        return image_set_index

    def _label_path_from_index(self):
        label_file = os.path.join(self.label_path, 'gt.pkl')
        assert os.path.exists(label_file), f'Path does not exist: {label_file}'
        with open(label_file, 'rb') as gt_file:
            try:
                label_data = pickle.load(gt_file)
            except:
                gt_file.seek(0)
                label_data = pickle.load(gt_file, encoding='latin1')
        return label_data

# Collate function for dataloader
def collate_fn(batch):
    images, labels, lengths = zip(*batch)
    images = torch.stack(images, 0)
    labels = torch.stack(labels, 0)
    lengths = torch.LongTensor(lengths)
    return images, labels, lengths

class SimplifiedCRNN(nn.Module):
    def __init__(self, num_classes, num_hidden=256, num_lstm_layers=2, dropout=0.3):
        super(SimplifiedCRNN, self).__init__()


        self.cnn = nn.Sequential(
            # Layer 1
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),
            nn.Conv2d(512, 512, kernel_size=2, padding=0),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout)
        )

        self.rnn = nn.LSTM(
            input_size=512,  # Matches the output channels from CNN
            hidden_size=num_hidden,
            num_layers=num_lstm_layers,
            bidirectional=True,
            batch_first=False
        )
        self.fc = nn.Linear(num_hidden * 2, num_classes)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # CNN forward
        x = self.cnn(x)

        # Prepare for RNN (B, C, H, W) -> (W, B, C*H)
        batch_size = x.size(0)
        x = x.permute(0, 3, 1, 2)  # (B, W, C, H)
        seq_len = x.size(1)
        x = x.reshape(batch_size, seq_len, -1)  # (B, W, C*H)
        x = x.permute(1, 0, 2)  # (W, B, C*H)

        # RNN forward
        self.rnn.flatten_parameters()
        rnn_out, _ = self.rnn(x)

        # Linear projection to classes
        output = self.fc(rnn_out)  # (seq_len, batch, num_classes)

        # Return log_softmax for CTC loss
        return F.log_softmax(output, dim=2)

# CTC decoder for evaluation
def ctc_decode(pred, blank=0):
    """Basic greedy CTC decoding"""
    ret = []
    for i in range(len(pred)):
        if pred[i] != blank and (len(ret) == 0 or pred[i] != ret[-1]):
            ret.append(pred[i])
    return ret

# Accuracy calculation
def compute_accuracy(outputs, labels, label_lengths, batch_size, seq_length):
    # Detach outputs before converting
    outputs = outputs.detach().permute(1, 0, 2)  # (B, T, C)
    preds = outputs.argmax(dim=2)  # (batch, seq_len)

    hit = 0.0
    total = 0.0

    for i in range(batch_size):
        pred = preds[i].cpu().numpy()
        label = labels[i][:label_lengths[i]].cpu().numpy()

        # Apply CTC decoding
        decoded_pred = ctc_decode(pred)

        # Check if prediction matches ground truth
        if len(decoded_pred) == len(label):
            match = True
            for k in range(len(decoded_pred)):
                if decoded_pred[k] != int(label[k]):
                    match = False
                    break
            if match:
                hit += 1.0
        total += 1.0

    return hit / total if total > 0 else 0.0

# Get character classes
def get_character_set():
    classes = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9",
               "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M",
               "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]

    lowercase = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
                "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"]
    classes.extend(lowercase)

    return classes

# Main training function
def main():
    # Configuration
    batch_size = 32
    seq_length = 25
    num_hidden = 512
    num_lstm_layer = 2
    num_epoch = 50
    learning_rate = 0.001
    num_label = 9
    data_shape = (100, 32)  # (W, H)

    # Create directories
    os.makedirs('model', exist_ok=True)

    # Get classes
    classes = get_character_set()
    num_classes = len(classes) + 1  # +1 for CTC blank

    print(f"Using device: {device}")
    print(f"Number of classes: {num_classes}")

    # Create datasets
    train_dataset = OCRDataset(batch_size, classes, data_shape, num_label, train_flag=True)
    val_dataset = OCRDataset(batch_size, classes, data_shape, num_label, train_flag=False)

    if len(train_dataset) == 0:
        print("Training dataset is empty! Please check data generation.")
        return

    if len(val_dataset) == 0:
        print("Validation dataset is empty! Please check data generation.")
        return

    print(f"Training dataset size: {len(train_dataset)}")
    print(f"Validation dataset size: {len(val_dataset)}")

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )

    # Create model - use simplified architecture
    model = SimplifiedCRNN(num_classes, num_hidden=num_hidden, num_lstm_layers=num_lstm_layer, dropout=0.3)
    model = model.to(device)

    # Define loss and optimizer - Adam works better than SGD for CTC
    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)

    # Print table header
    print(f"{'Epoch':^5} | {'Train Loss':^10} | {'Train Acc':^9} | {'Val Loss':^10} | {'Val Acc':^9} | {'Best':^5}")
    print("-" * 55)

    # Training loop
    best_val_acc = 0.0

    for epoch in range(1, num_epoch + 1):
        # Train
        model.train()
        train_loss = 0.0
        train_acc = 0.0
        train_samples = 0

        for batch_idx, (data, target, target_lengths) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)

            input_lengths = torch.full((data.size(0),), output.size(0), dtype=torch.long, device=device)

            # Calculate CTC loss
            loss = criterion(output, target, input_lengths, target_lengths)

            # Backward pass
            loss.backward()

            # Gradient clipping to prevent explosion
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

            optimizer.step()

            # Calculate metrics
            batch_size = data.size(0)
            train_loss += loss.item() * batch_size
            train_samples += batch_size

            # Calculate accuracy (only occasionally to save time)
            if batch_idx % 5 == 0:
                acc = compute_accuracy(output, target, target_lengths, batch_size, output.size(0))
                train_acc += acc * batch_size

        train_loss /= train_samples
        train_acc /= train_samples

        # Validate
        model.eval()
        val_loss = 0.0
        val_acc = 0.0
        val_samples = 0

        with torch.no_grad():
            for data, target, target_lengths in val_loader:
                data, target = data.to(device), target.to(device)

                output = model(data)

                input_lengths = torch.full((data.size(0),), output.size(0), dtype=torch.long, device=device)

                loss = criterion(output, target, input_lengths, target_lengths)

                batch_size = data.size(0)
                val_loss += loss.item() * batch_size
                val_samples += batch_size

                acc = compute_accuracy(output, target, target_lengths, batch_size, output.size(0))
                val_acc += acc * batch_size

        val_loss /= val_samples
        val_acc /= val_samples

        # Save best model
        is_best = val_acc > best_val_acc
        best_val_acc = max(val_acc, best_val_acc)

        # Print progress
        best_mark = "*" if is_best else ""
        print(f"{epoch:^5d} | {train_loss:^10.4f} | {train_acc:^9.4f} | {val_loss:^10.4f} | {val_acc:^9.4f} | {best_mark:^5}")

        if is_best:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
            }, os.path.join('model', 'crnn_best.pth'))

    # Save final model
    torch.save(model.state_dict(), os.path.join('model', 'crnn_final.pth'))
    print(f"\nTraining Complete! Best validation accuracy: {best_val_acc:.4f}")

if __name__ == "__main__":
    main()
